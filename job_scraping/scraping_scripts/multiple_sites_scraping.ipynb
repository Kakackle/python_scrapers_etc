{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26b07689-4581-4ab5-8b92-dab3696fb9c2",
   "metadata": {},
   "source": [
    "## Scraping jobs listing from multiple sites over multiple pages and save them to separate .csv files first, then try to combine them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f81d28d2-137f-4d7d-83f8-23a6ad6f2dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from pydantic import BaseModel\n",
    "from typing import List, Optional\n",
    "from datetime import datetime, date\n",
    "import re\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da70ba7-1bde-4b1b-8075-2a0f467689d3",
   "metadata": {},
   "source": [
    "### Specify the search term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "836700fb-73ec-4760-abb2-9be26371d5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_term = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6bcf7c8-5d7f-4ca0-89dd-607fac3130ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1\n",
    "urls = [\n",
    "    'https://www.careerjet.pl/{search_term}-praca.html?radius=0&p={page}&sort=date',\n",
    "    'https://it.pracuj.pl/praca/{search_term};kw?sc=0&pn={page}'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5d43f80-d09e-42c5-bc57-fd9993b49f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = [\n",
    "    'careerjet',\n",
    "    'pracuj'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4601e7-b21b-4c14-ae7c-6e9305f1f5a1",
   "metadata": {},
   "source": [
    "### Model for listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80a745b9-2a75-4f99-9ea9-0bf9fde32747",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Listing(BaseModel):\n",
    "    title: str\n",
    "    link: str\n",
    "    company: str = None\n",
    "    location: str = None\n",
    "    salary: str = None\n",
    "    description: str = None\n",
    "    date_added: str | datetime = None\n",
    "    skills: List[str] = None\n",
    "    search_term: str = 'python'\n",
    "    date_searched: date = date.today()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3d19c1-a45b-4a34-940e-cf1b23dc3bc0",
   "metadata": {},
   "source": [
    "### Data extraction functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56ffb96-077b-46f6-ab50-aeef03c98c58",
   "metadata": {},
   "source": [
    "#### careerjet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17b7334e-a183-4e28-868b-06be4a24fb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_careerjet(soup):\n",
    "    page_objects = []\n",
    "    try:\n",
    "        listings = soup.find('ul', attrs={'class': 'jobs'})\n",
    "        listings_arr = listings.find_all('article', attrs={'class': 'job'})\n",
    "    except:\n",
    "        return page_objects\n",
    "    for listing in listings_arr:\n",
    "        try:\n",
    "            listing_link = listing.get('data-url')\n",
    "            link = f'careerjet.pl{listing_link}'\n",
    "        except:\n",
    "            link = ''\n",
    "            \n",
    "        try:\n",
    "            title = listing.header.h2.getText().strip()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            company = listing.find('p', attrs={'class': 'company'}).getText()\n",
    "        except:\n",
    "            company = ''\n",
    "\n",
    "        try:\n",
    "            location = listing.find('ul', attrs={'class':'location'}).li.getText().strip()\n",
    "        except:\n",
    "            location = ''\n",
    "\n",
    "        try:\n",
    "            description = listing.find('div', attrs={'class':'desc'}).getText().strip()\n",
    "        except:\n",
    "            description = ''\n",
    "        \n",
    "        obj = Listing(title = title, link = link, company = company,\n",
    "                 description = description, location = location)\n",
    "        page_objects.append(obj)\n",
    "    print(f'careerjet page {page} objects: ', len(page_objects))\n",
    "    return page_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e381967-42f8-4ad3-b0ee-e53a7d7bd145",
   "metadata": {},
   "source": [
    "#### pracuj.pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa0f45ce-4bd2-4dce-8266-47755d22c139",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regexes\n",
    "pracuj_date = re.compile(r'Opublikowana: (.*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5682caa-2a6a-4436-8907-d4374b1d5635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pracuj(soup):\n",
    "    page_objects = []\n",
    "    try:\n",
    "        listings = soup.find('div', attrs={'data-test': 'section-offers'})\n",
    "        listings_arr = listings.find_all(class_='c1fljezf')\n",
    "    except:\n",
    "        return page_objects\n",
    "    \n",
    "    for listing in listings_arr:\n",
    "        try:\n",
    "            listing_link = listing.find('a', attrs={'data-test': 'link-offer'}).get('href')\n",
    "        except:\n",
    "            listing_link = ''\n",
    "        \n",
    "        try:\n",
    "            listing_title = listing.find('h2', attrs={'data-test': 'offer-title'}).a.getText()\n",
    "        except:\n",
    "            listing_title = ''\n",
    "    \n",
    "        try:\n",
    "            listing_date = listing.find('p', attrs={'data-test':'text-added'}).getText()\n",
    "            listing_date = pracuj_date.match(listing_date)[0]\n",
    "        except:\n",
    "            listing_date = ''\n",
    "    \n",
    "        try:\n",
    "            listing_skills = []\n",
    "            skills = listing.find_all('span', attrs={'data-test': 'technologies-item'})\n",
    "            for skill in skills:\n",
    "                skill = skill.getText()\n",
    "                listing_skills.append(skill)\n",
    "        except:\n",
    "            listing_skills = []\n",
    "    \n",
    "        try:\n",
    "            listing_company = listing.find('a', attrs={'data-test':'text-company-name'}).getText()\n",
    "        except:\n",
    "            listing_company = ''\n",
    "    \n",
    "        try:\n",
    "            listing_location = listing.find('h5', attrs={'data-test':'text-regon'}).getText()\n",
    "        except:\n",
    "            listing_location = ''\n",
    "    \n",
    "        try:\n",
    "            listing_description = listing.find('span', attrs={'class':'t126uk2l'}).getText()\n",
    "        except:\n",
    "            listing_description = ''\n",
    "    \n",
    "        try:\n",
    "            listing_salary = listing.find('span', attrs={'data-test':'offer-salary'}).getText()\n",
    "        except:\n",
    "            listing_salary = ''\n",
    "            \n",
    "        obj = Listing(title = listing_title, link = listing_link,\n",
    "                      date_added = listing_date, skills = listing_skills,\n",
    "                     company = listing_company, location = listing_location,\n",
    "                     description = listing_description, salary  = listing_salary)\n",
    "        page_objects.append(obj)\n",
    "        \n",
    "    print(f'pracuj.pl page {page} objects: ', len(page_objects))\n",
    "    return page_objects   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca6ea455-9f70-4541-a2d9-77d668bf5f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractors = [\n",
    "    extract_careerjet,\n",
    "    extract_pracuj\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab27b7fc-c89e-41ed-9134-d81ac16edfe8",
   "metadata": {},
   "source": [
    "### Create appropriate folders for saving data\n",
    "search_term folder -> date_folder -> combined/individual/fluff -> files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12ce447d-2ae4-4a0a-a9e8-f06a2032eff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-11-28\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "today = date.today()\n",
    "print(today)\n",
    "path = r'../scraping_results/' + search_term + '/' + str(today)\n",
    "if not os.path.exists(path):\n",
    "    os.makedirs(path)\n",
    "path += '/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f64ccd2-c8b6-4145-b013-54e130e63f5a",
   "metadata": {},
   "source": [
    "### Looping through the first 5 pages, within looping through the urls / providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae778ac-d81c-4f31-bf04-c8496aebd55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pages = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9085bcb3-05b4-400f-b871-34cdb53fab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "careerjet page 1 objects:  20\n",
      "careerjet page 2 objects:  20\n",
      "pracuj.pl page 1 objects:  50\n",
      "pracuj.pl page 2 objects:  50\n",
      "all listings:  140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14992\\1180034899.py:12: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  site_listings_pd = pd.DataFrame([obj.dict() for obj in listings_objects])\n"
     ]
    }
   ],
   "source": [
    "listings_objects = []\n",
    "for url_index, url in enumerate(urls):\n",
    "    site_listings = []\n",
    "    for page in range(1,no_pages+1):\n",
    "        page_url = url.format(search_term=search_term, page=page)\n",
    "        page_response = requests.get(page_url)\n",
    "        page_html = page_response.text\n",
    "        page_soup = bs(page_html, 'html.parser')\n",
    "        page_objects = extractors[url_index](page_soup)\n",
    "        site_listings.extend(page_objects)\n",
    "    # save listings for one site only\n",
    "    site_listings_pd = pd.DataFrame([obj.dict() for obj in listings_objects])\n",
    "    path_ind = path + r'individual'\n",
    "    if not os.path.exists(path_ind):\n",
    "        os.makedirs(path_ind)\n",
    "    path_ind += '/'\n",
    "    site_listings_pd.to_csv(path_ind + f'{sites[url_index]}_{search_term}_{no_pages}pages_{today}.csv')\n",
    "    listings_objects.extend(site_listings)\n",
    "print('all listings: ', len(listings_objects))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53801ee4-894f-4e3f-8d86-f6cddbc6d06f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        title  \\\n",
      "0               Monitoring Specialist (m/f/d)   \n",
      "1                             DevOps Engineer   \n",
      "2  Support Technical Account Manager Cracow 2   \n",
      "3  Support Technical Account Manager Cracow 4   \n",
      "4             Senior Test Automation Engineer   \n",
      "\n",
      "                                                link               company  \\\n",
      "0  careerjet.pl/jobad/pldc3276bab0934fae6b8ed4a63...                Damovo   \n",
      "1  careerjet.pl/jobad/plf6bbc4809d8132eb7cb141aa7...  Check Point Software   \n",
      "2  careerjet.pl/jobad/plc11b328054f74b124c5bcce38...                         \n",
      "3  careerjet.pl/jobad/pl7b01a6ee86fab684983acb9ec...                         \n",
      "4  careerjet.pl/jobad/pl69754da3dc6d16cc5d7a88bf3...                  Hays   \n",
      "\n",
      "                location salary  \\\n",
      "0  Warszawa, mazowieckie   None   \n",
      "1  Warszawa, mazowieckie   None   \n",
      "2    Kraków, małopolskie   None   \n",
      "3    Kraków, małopolskie   None   \n",
      "4    Kraków, małopolskie   None   \n",
      "\n",
      "                                         description date_added skills  \\\n",
      "0  Full Time  Warsaw, Poland  Hybrid  With Profes...       None   None   \n",
      "1  Why Join Us?   Check Point is seeking a highly...       None   None   \n",
      "2  Our Support TAM fulfills a necessary role in o...       None   None   \n",
      "3  Our Support TAM fulfills a necessary role in o...       None   None   \n",
      "4  Senior Test Automation Engineer   lokalizacja:...       None   None   \n",
      "\n",
      "  search_term date_searched  \n",
      "0      python    2023-11-28  \n",
      "1      python    2023-11-28  \n",
      "2      python    2023-11-28  \n",
      "3      python    2023-11-28  \n",
      "4      python    2023-11-28  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_14992\\3227278432.py:1: PydanticDeprecatedSince20: The `dict` method is deprecated; use `model_dump` instead. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.5/migration/\n",
      "  listings_pd = pd.DataFrame([obj.dict() for obj in listings_objects])\n"
     ]
    }
   ],
   "source": [
    "listings_pd = pd.DataFrame([obj.dict() for obj in listings_objects])\n",
    "print(listings_pd.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9e1e700-49ce-4c28-9d27-6821cdab3e8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'link', 'company', 'location', 'salary', 'description',\n",
       "       'date_added', 'skills', 'search_term', 'date_searched'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listings_pd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ddecf6e-16b5-4c6e-87bb-8bc91b0a14a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_string = '_'.join(sites)\n",
    "path_comb = path + 'combined'\n",
    "if not os.path.exists(path_comb):\n",
    "    os.makedirs(path_comb)\n",
    "path_comb += '/'\n",
    "listings_pd.to_csv(path_comb + f'{sites_string}_{search_term}_{today}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c785958-63a0-415d-ac17-ed9d384c6f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# str(today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7fcdcd34-f1d2-429a-89f3-2a2a689ad1bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./scraping_results/python/2023-11-28/individual/\n",
      "./scraping_results/python/2023-11-28/combined/\n"
     ]
    }
   ],
   "source": [
    "print(path_ind)\n",
    "print(path_comb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
