1. ~~zacznij od jednej strony zbieranie~~
2. ~~zapisz w csv jak olx soup~~
3. ~~wiecej stron i do osobnych plikow~~
4. TODO: dodaj do zapisywanych danych search term i dzien scrapowania
5. TODO: przekonwertuj juz istniejace scrapy na to i podpisz pliki zaleznie od strony, daty i search term. i ile stron sciagniete?
6. TODO?: wiecej stron
7. TODO: polaczenie z plikow w jeden dataframe:
    1. Brudny plik / df z duplikatami
    2. Plik z duplikatami usunietymi jesli roznia sie tylko data scrapu, czyli tak jakby czysty
    3. Plik z usunietymi duplikatami gdzie zgadza sie tytul i firma
    4. .
    5. Kwestia wiele search term, a ta sama oferta
    6. 
9. ~~TODO: model na zbierane dane z pydantic?~~
10. TODO: Wstepne wizualizacje:
    1. Zliczanie ofert zaleznie od daty i jaki bar, scatter etc
    2. Wordcloud z tytulow, jesli mozliwe to opisow
    3. Zliczanie wystepujacych skilli tam gdzie mozliwe - pracuj
    4. Salary tam gdzie mozliwe - fluff
    5. Zliczanie ofert po firmach
    6. Tak samo po lokacjach itd
    7. Moze nawet jakas mapka polski i na niej rysowane lokacje? w sensie male/duze kropki itd
    8. 
12. TODO: Jakos zeby uruchomialo sie codziennie i zbieralo i aktualizowalo wszystko?
13. TODO: Jesli zbierane z roznych stron i z roznych plikow to zapisywane jakos w pliki podpisane datami itd zeby odrozniac, moc porownwyac dni i inne

14. TODO: https://www.youtube.com/watch?v=xzxWLVCUvLo - wykorzystanie w pelni pydantic pod katem walidacji danych zbieranych, co bedzie dokonywane przy tworzeniu juz

Possible job sites:

https://it.pracuj.pl/praca/python;kw?sc=0

https://justjoin.it/?keyword=python&orderBy=DESC&sortBy=newest

https://nofluffjobs.com/Python?page=1&sort=newest

https://www.careerjet.pl/szukaj/oferty_pracy?s=python&l=Polska&radius=0&sort=date

